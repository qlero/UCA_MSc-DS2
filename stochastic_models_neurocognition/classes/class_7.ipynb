{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4dc665ff",
   "metadata": {},
   "source": [
    "# Stochastic Models in Neurocognition\n",
    "\n",
    "## Class 7 - STATISTICS FOR PROCESSES\n",
    "\n",
    "<hr>\n",
    "\n",
    "**Preliminary Notes**:\n",
    "\n",
    "Homework to send to Josue & Etienne before January 9th.\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b7c08",
   "metadata": {},
   "source": [
    "## 1 - Likelihood\n",
    "\n",
    "For a given model, $X\\sim\\mathbb{P}_\\theta$, $g_\\theta(x)=\\mathbb{P}_\\theta(X=x)$, one can compute the likelihood: $$\\theta\\rightarrow\\mathcal{L}_\\theta(X)=\\mathbb{P}_\\theta(X=x)|_{x=X}$$ \n",
    "\n",
    "The MLE is $\\hat{\\theta}=\\underset{\\theta\\in\\Theta}{argmax}\\,\\,\\mathcal{L}_\\theta(X)=\\underset{\\theta\\in\\Theta}{argmax}\\,\\,\\mathcal{l}_\\theta(X)$ with $l$ the log-likelihood.\n",
    "\n",
    "One need $\\theta$ to be the *minimal possible set of parameters*. \n",
    "\n",
    "### 1.1 Markov Chain\n",
    "\n",
    "**Setup**: Markov chain with two states $X_0, X_1, ..., X_n$ with values $\\{0,1\\}$. We have full knowledge of the setup if we know $P=P(X_0 = 0)$ and $P_{0\\rightarrow1}$ and $P_{1\\rightarrow0}$.\n",
    "\n",
    "**Example**: Binned spike trains (e.g. 0001000100100110..., at least one spike has happened at step 4) with a refractory period (after a spike, a neuron is less likely to produce another spike). \n",
    "\n",
    "<span style=\"color:red\">ADD GRAPH (state 0 to 1 and back)</span>\n",
    "\n",
    "**Parameter space:**\n",
    "\n",
    "$$\\theta = (P, P_{0\\rightarrow1}, P_{1\\rightarrow0})$$\n",
    "\n",
    "For a given realisation $x_0,...,x_n$:\n",
    "\n",
    "\\begin{align}\n",
    "P_\\theta((X_0, ..., X_n) &= (x_0,...,x_n)) = P(X_0=x_0, ..., X_n=x_n)\\\\\n",
    "&=P_\\theta(X_0=x_0, ..., X_n-1=x_n-1).P_\\theta(X_n=x_n|\\text{all the point from 0 to n-1})\\quad\\text{markov property: $P_\\theta(X_n=x_n|X_{n-1}=x_{n-1})$}\\\\\n",
    "&=P_\\theta(X_0=x_0)*...*P_\\theta(X_n=x_n|X_{n-1}=x_{n-1})\\\\\n",
    "&=p^{x_0}(1-p)^{1-x_0}.p_{0\\rightarrow1}^{n_{0\\rightarrow1}}.p_{1\\rightarrow0}^{n_{1\\rightarrow0}}.p_{0\\rightarrow0}^{n_{0\\rightarrow0}}.p_{1\\rightarrow1}^{n_{1\\rightarrow1}}\\\\\n",
    "\\end{align}\n",
    "\n",
    "With $n_{i\\rightarrow j}$ the number of transition from i to j seen in the sequence $x_0,...,x_n$.\n",
    "\n",
    "**Likelihoods:**\n",
    "\n",
    "So the likelihood of a sequence $X_0,...,X_n$ with counts $N-{i\\rightarrow j}$ is the number of times a transition $i\\rightarrow j$ occurs in the sequence is:\n",
    "\n",
    "$$L_\\theta(X)=p^{X_0}(1-p)^{1-X_0}.p_{0\\rightarrow1}^{n_{0\\rightarrow1}}.(1-p_{0\\rightarrow1})^{n_{1\\rightarrow0}}.p_{1\\rightarrow0}^{n_{0\\rightarrow0}}.(1-p_{1\\rightarrow0})^{n_{1\\rightarrow1}}$$\n",
    "\n",
    "The Log-likelihood is:\n",
    "\n",
    "$$l_\\theta(X) = X_0log(p) + (1-X_0)log(1-p) + N_{1\\rightarrow0}log(p_{1\\rightarrow0}) + N_{1\\rightarrow1}log(1 - p_{1\\rightarrow0}) + N_{0\\rightarrow0}log(1 - p_{0\\rightarrow1}) + N_{0\\rightarrow1}log(p_{0\\rightarrow1})$$ \n",
    "\n",
    "\n",
    "<span style=\"color:red\">FINISH FORMULAS ABOVE</span>\n",
    "\n",
    "**MLE of the transition**:\n",
    "\n",
    "$\\frac{\\delta l_\\theta(x)}{\\delta p_{1\\rightarrow0}} = \\frac{N_{1\\rightarrow0}}{p_{1\\rightarrow0}} - \\frac{N_{1\\rightarrow1}}{1 - p_{1\\rightarrow0}}$\n",
    "\n",
    "<span style=\"color:red\">ADD FORMULA + Explanation</span>\n",
    "\n",
    "Hence:\n",
    "\n",
    "$$\\hat{p}_{1\\rightarrow0} = \\frac{N_{1\\rightarrow0}}{N_{1\\rightarrow1}+N_{1\\rightarrow0}}$$\n",
    "\n",
    "i.e. the number of times \"1 to 0\" occured divided by the number of times \"1\" was seen. Similarly:\n",
    "\n",
    "$$\\hat{p}_{0\\rightarrow1} = \\frac{N_{0\\rightarrow1}}{N_{0\\rightarrow1}+N_{0\\rightarrow0}}$$\n",
    "\n",
    "In general (for tutorial), **without further constraint, the MLE for the transition i to j is equal to** $\\frac{N_{i\\rightarrow j}}{\\#\\{i\\rightarrow j\\}}$\n",
    "\n",
    "<span style=\"color:red\">vERIFY FORMULA</span>\n",
    "\n",
    "> This estimator is consistent if the chain is recurrent. Recurrence means that the chain will visit the transition an infinite number of times when $n\\rightarrow+\\infty$. \n",
    "\n",
    "<span style=\"color:red\">ADD GRAPH</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8d9f5",
   "metadata": {},
   "source": [
    "Usually when seeing a graph, one can guess what are the recurrent states\n",
    "\n",
    "<span style=\"color:red\">ADD GRAPH</span>\n",
    "\n",
    "Behind the fact that $\\hat{p}_{i\\rightarrow j}\\underset{n\\rightarrow+\\infty}{\\rightarrow}p_{i\\rightarrow j}$ if i and j are recurrent, then you have the erg <span style=\"color:red\">FINISH</span>\n",
    "\n",
    "### About Continuous Markov Processes with discrete states\n",
    "\n",
    "<span style=\"color:red\">ADD GRAPH SLIDE 9</span>\n",
    "\n",
    "The MLE is $$\\hat{\\lambda}_{i\\rightarrow j} = \\frac{1}{\\text{mean duration of being in state i and jumping to state j}}$$\n",
    "It is consistant if the chain is recurrent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1cb5d0",
   "metadata": {},
   "source": [
    "### 1.2 Point processes\n",
    "\n",
    "We observe a point process $N$ with conditional intensity $\\lambda(.)$ that depends only on the previous point of $N$. If we observe $N=\\{t_1,...,t_n\\}$ with $T_{max}$. \n",
    "\n",
    "<span style=\"color:red\">ADD GRAPH + formula slide 10</span>\n",
    "\n",
    "thanks to thinning, one knows that $N$ can be generated as follows:\n",
    "\n",
    "<span style=\"color:red\">ADD slide 11, 12</span>\n",
    "\n",
    "So the likelihood of $N$ is:\n",
    "\n",
    "\\begin{align}\n",
    "L_\\lambda(N) &= \\prod_{T\\in N}\\lambda(t).e^{-\\int_0^{T_{max}}\\lambda(t)dt}\\\\\n",
    "\\end{align}\n",
    "\n",
    "The log-likelihood of $N$ is:\n",
    "\n",
    "\\begin{align}\n",
    "l_\\lambda(N) &= \\sum_{T\\in N}log(\\lambda(t))-\\int_0^{T_{max}}\\lambda(t)dt\\\\\n",
    "&= \\int_0^{T_{max}}log(\\lambda(t))dN_t-\\int_0^{T_{max}}\\lambda(t)dt\\\\\n",
    "\\end{align}\n",
    "\n",
    "<u>**Homogeneous Poisson Process on $[0,T_{max}]$**</u>\n",
    "\n",
    "We observe $N$ a Poisson process with intensity (fixed) $\\theta\\in\\mathbb{R}_+$. The conditional intensity is therefore $\\lambda(t)=\\theta$. \n",
    "\n",
    "\\begin{align}\n",
    "l_\\lambda(N)&= \\int_0^{T_{max}}log(\\lambda(t))dN_t-\\int_0^{T_{max}}\\lambda(t)dt\\\\\n",
    "l_\\theta(N)&=log(\\theta)N_{[0,T_{max}]}-\\theta T_{max}\\\\\n",
    "\\frac{\\delta l_\\theta}{\\delta\\theta}&= \\frac{1}{\\theta}N_{[0,T_{max}]}-T_{max}\n",
    "\\end{align}\n",
    "\n",
    "We verify the signs of the derivative and:\n",
    "\n",
    "$$\\hat{\\theta}=\\frac{N_{[0,T_{max}]}}{T_{max}}$$\n",
    "\n",
    "This is the **classical estimator of the firing rate**.\n",
    "\n",
    "<u>**Non-homogeneous Poisson Process on $[0,T_{max}]$**</u>\n",
    "\n",
    "If we see $N_1, ..., N_n$ n IID Poisson process with intensity $f(t)$, then $N_1\\cup...\\cup N_n$ is a Poisson process with intensity $nf(t)$ (the sum of the intensities). \n",
    "\n",
    "The asymptotic here is when $n\\rightarrow+\\infty$ because one will have an increasing amount of points everywhere.\n",
    "\n",
    "<span style=\"color:red\">ADD GRAPH + formulas below</span>\n",
    "\n",
    "\\begin{align}\n",
    "l_\\lambda(N) &= \\int_0^{T_{max}}log(\\lambda(t))dN_t-\\int_0^{T_{max}}\\lambda(t)dt\\\\\n",
    "l_{[a, b]}(N) &= log(na)N_{[0,\\frac{Tmax}{2}]} + log(nb)N_{]\\frac{Tmax}{2}, Tmax]}-na\\frac{Tmax}{2} - nb\\frac{Tmax}{2}\\\\\n",
    "\\hat{a} &=\\\\\n",
    "\\hat{b} &=\n",
    "\\end{align}\n",
    "\n",
    "One can also select models in general:\n",
    "\n",
    "- by using AIC: $m\\in\\mathcal{M}$ with $d_m$ degrees of freedom then $\\hat{m}=\\underset{m\\in\\mathcal{M}}{argmin}\\, -l_m(N) + d_m$\n",
    "- or if it is possible find out with iid trials with cross-validation\n",
    "\n",
    "<u>Remark</u> log-likelihood is not the only contrast for point processes. Least Square exists, this is: $$-2\\int^{Tmax}_0\\lambda(t)dN_t + \\int_0^{Tmax}\\lambda(t)^2dt$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5418d11",
   "metadata": {},
   "source": [
    "## 2 -Time rescaling theorem and the goodness-of-fit tests\n",
    "\n",
    "### Time rescaling theorem (also time change theorem, Wattanabe theorem)\n",
    "\n",
    "> If $N$ is a point process of conditional intensity $\\lambda(t)$ with **compensator** $\\Lambda(t)=\\int_0^t\\lambda(u)du$ on $[0,Tmax]$ then the process $N'=\\{\\Lambda(T)\\text{ for }T\\in N\\}$ is a Poisson process of rate 1 on $[0,\\Lambda(Tmax)]$\n",
    "\n",
    "<u>Remark:</u>\n",
    "\n",
    "$t\\rightarrow N_t=N_{[0,Tmax]}$, a counting process. increasing. Then $t\\rightarrow \\Lambda(t) = \\int_0^{Tmax}\\lambda(u)du$. $\\Lambda(t)$ is a compensator because $M_t = N_t - \\Lambda(t)$ is a martingale. This means that $\\mathbb{E}[M_t|\\mathcal{F}_S]=M_S\\text{ for }s<t$ with $\\mathcal{F}_S$ the past until $S$.\n",
    "\n",
    "$$\\mathbb{E}[M_t|\\mathcal{F}_0]=\\mathbb{E}[M_t]=M_0 = N_0-\\Lambda(0) = 0$$\n",
    "\n",
    "$M_t$ is centered.\n",
    "\n",
    "The compensator will follow $N_t$ more closely than its expectation -> \"super centered\" noise for the difference $N_t-\\Lambda(t)$. Moreover:\n",
    "\n",
    "$$\\mathbb{E}[n_{[0, t]}] = \\mathbb{E}[n_{[0, \\Lambda(t)]}] = E[N_t] - E[\\Lambda(t)]$$\n",
    "\n",
    "the mean number of points on $[0, \\Lambda(t)]$ is roughly $\\Lambda(t)$ -> rate 1.\n",
    "\n",
    "### Ogada's tests\n",
    "\n",
    "I observe a process $N$ on $[0, Tmax]$ with intensity $\\lambda$. I want to test whether $H_0:\\lambda(t) = \\lambda_0(t)$. \n",
    "- One computes the candidate compensator $\\Lambda_0(t)=\\int^t_0\\lambda_0(t)dt$\n",
    "- One computes $N'=\\{\\Lambda_0(t), T\\in N\\}$\n",
    "- Under $H_0$, $N'$ should be a Poisson process with rate 1. \n",
    "\n",
    "$K$ tests are performed on the $N'$ points (with $K=2+L$) in order to check for the null hypothesis.\n",
    "\n",
    "1) **First test**\n",
    "\n",
    " The points are uniformly distributed on $[0, \\Lambda_0(Tmax)]$: Kolmogorov Smirnov test of uniformityon the points of Tmax\n",
    " \n",
    "```R\n",
    "ks.test\n",
    "```\n",
    " \n",
    "2) **Second test**\n",
    " \n",
    "The delays between points are exponential with parameter 1: Kolmogorov Sirnov of exponentiality on the delays\n",
    "\n",
    "3) **Third test**\n",
    "\n",
    "Independence between the delays: autocorreletion tests with different lags\n",
    "\n",
    " \n",
    "```R\n",
    "acf.(,ci=1-0.05/K) # where K is the number of test\n",
    "```\n",
    "\n",
    "Be careful, at the end you have $2 + l$ test (with $l$ the number of lags), so correct with Bonferroni the level of the p-value (thus the division by K).\n",
    "\n",
    "> **We reject either if the p-values in a or b are smaller than 0.05/K, or if c results in one of the computed correlation falling outside of the confidence interval on a visualization graph**\n",
    "\n",
    "**In practice (simulation)** -- one have a new algorithm to simulate N with intensity $\\lambda$. To check that it works, one computes $\\Lambda(t)=\\int_0^t\\lambda(u)du$, then $N'=\\{\\Lambda(t), T\\in N\\}$, then Ogada's tests on $N'$. One wants large p-values to not reject.\n",
    "\n",
    "**In practice (Real data)** -- one observe $N$. One thinks the model $\\lambda_\\theta$ is good but you donÂ´t know $\\theta$ and you want to verify it. \n",
    "- Then $\\hat{\\theta}$ (MLE, etc.) please verify in simulation of the model that $\\hat{\\theta}$ is a good estimator for the model.\n",
    "- $\\Lambda(t) = \\int_0^t\\lambda_\\hat{\\theta}(u)du$\n",
    "- $\\hat{N}'=\\{\\hat{\\Lambda}(T), t\\in N\\}$\n",
    "- Ogada's tests on $\\hat{N}'$\n",
    "\n",
    "Be careful this test is conservative:\n",
    "- if you reject: really there is a problem with the model\n",
    "- if you accept, we you don't know"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f67201",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
