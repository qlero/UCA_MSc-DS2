---
title: "Point Processes"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

<hr>

## Question 1 - Maximum likelihood for Markov chain 

Given a cognitive context of sleeping patterns, a check is performed every five minutes on study participants and their state is noted. We want to model this by a (discrete) Markov chain with 3 states:
- "sleeping"
- "(fully) awake"
- "drowsy"

**Step A** -- Plot the diagrams and parametrize the two following (informal) models with the least
possible amount of parameters
- You can only pass from "fully awake" to "sleeping" through the state "drowsy" and
vice-versa: you can only go from sleeping to "fully awake" through "drowsy".
- It is possible (but not necessary) to be instantly awake or instantly sleeping and to
skip the "drowsy" state.

**Step B** -- Make a function in R which simulates the first model if you give in entries the state $x_0$,
the parameters of the model (only the ones parametrizing the transitions) and the length of the chain to simulate $n$.

**Step C** -- Write the (log)-likelihood of this problem and give the maximum likelihood estimator of $P_{D\rightarrow S}$, the probabilty to fall asleep when you are drowsy. Would it be different if we were in the second model?

**Step D** -- Show on simulation that this estimator converges towards the true probability when n tends to infinity.

**Step E** -- How would you decide if we are in the first or second model when you have the observations?

### STEP A

The first model is defined as in picture 1 attached, with a **minimum of four parameters** such that:

\begin{align}
\text{parameter space}&\text{:}\quad\{P_{A\rightarrow D},P_{S\rightarrow D},P_{D\rightarrow A},P_{D\rightarrow S})\}\\
P_{A\rightarrow A}&=1-P_{A\rightarrow D}\\
P_{S\rightarrow S}&=1-P_{S\rightarrow D}\\
P_{D\rightarrow D}&=1-P_{D\rightarrow A}-P_{D\rightarrow S}\\
\end{align}

With $A$ the "awake" state, $S$ the "sleeping" state, and $D$ the "drowsy" state.

**Of note**, if the selection of the state at time $T=0$ (i.e. the start state) is also random, we have to add 2 more parameters: $P(x_0=A)$ and $P(x_0=S)$ such that $P(x_0=D)=1-P(x_0=A)-P(x_0=S)$. As such, it would bring model 1's **number of parameters to six**.

![model1](images/model1.png)

The second model is defined as in picture 2 attached, with a **minimum of six parameters** such that:

\begin{align}
\text{parameter space}&\text{:}\quad\{P_{A\rightarrow D},P_{A\rightarrow S},P_{S\rightarrow D},P_{S\rightarrow A},P_{D\rightarrow A},P_{D\rightarrow S})\}\\
P_{A\rightarrow A}&=1-P_{A\rightarrow D}\\
P_{S\rightarrow S}&=1-P_{S\rightarrow D}\\
P_{D\rightarrow D}&=1-P_{D\rightarrow A}-P_{D\rightarrow S}\\
\end{align}

**Of note**, if the selection of the state at time $T=0$ (i.e. the start state) is also random, we have to add 2 more parameters: $P(x_0=A)$ and $P(x_0=S)$ such that $P(x_0=D)=1-P(x_0=A)-P(x_0=S)$. As such, it would bring model 2's **number of parameters to eight**.

![model2](images/model2.png)

### STEP B

```{r generate_model_1}

compute_jump <- function(state, p_AtoD, p_StoD, p_DtoS, p_DtoA){
  ### Computes the next jump
  if (state == "A"){
    draw = rbinom(1, 1, p_AtoD)
    if (draw == 1){
      return("D")
    } else {
      return("A")
    }
  }
  if (state == "S") {
    draw = rbinom(1, 1, p_StoD)
    if (draw == 1){
      return("D")
    } else {
      return("S")
    }
  }
  # Else (State is drowsy)
  draw = runif(1, 0, 1)
  if (draw < p_DtoS){
      return("S")
    } else if (draw < p_DtoS + p_DtoA) {
      return("A")
    } else {
      return("D")
    }
}

generate_process_from_model1 <- function(x0, n, p_AtoD, p_StoD, p_DtoS, p_DtoA){
  ### Generates a process given the model 1 previously discribed
  ### x0: start state, p_AtoD: jump probability from A to D, 
  ### p_StoD: jump probability from S to D, n: process length
  if (!x0 %in% c("A","S","D")) {
    print("Wrong input for entry state x0")
    return(0)
  }
  process = c(x0)
  for (step in c(1:n)) {
    x0 = compute_jump(x0, p_AtoD, p_StoD, p_DtoS, p_DtoA)
    process = c(process, x0)
  }
  return(process)
}

```

We produce a couple examples:

```{r generated_model1_example1}

p_AtoD = 0.4
p_StoD = 0.2
p_DtoS = 0.4 
p_DtoA = 0.4
generate_process_from_model1("A", 50, p_AtoD, p_StoD, p_DtoS, p_DtoA)
generate_process_from_model1("S", 50, p_AtoD, p_StoD, p_DtoS, p_DtoA)
generate_process_from_model1("D", 50, p_AtoD, p_StoD, p_DtoS, p_DtoA)

```

### STEP C

Given the parameter space $\theta=\{P_{A\rightarrow D},P_{S\rightarrow D},P_{D\rightarrow A},P_{D\rightarrow S})\}$, for a given $n$-length realization of the model 1's process $x_0, ..., x_n$ is given by:

\begin{align}
P_\theta((X_0, ..., X_n)&=(x_0, ..., x_n))\\
&=P_\theta(X_0=x_0, ..., X_n=x_n)\\
&=P_\theta(X_0=x_0).P_\theta(X_1=x_1|X_0=x_0).\,...\,.P_\theta(X_n=x_n, ..., X_{n-1}=x_{n-1})\\
\end{align}

Given this setup, and including the parameters $P(x_0=A)$ and $P(x_0=S)$ such that $P(x_0=D)=1-P(x_0=A)-P(x_0=S)$ as noted in Step B, we can rewrite $P_\theta(X_0=x_0, ..., X_n=x_n)$ such that:

\begin{align}
P_\theta(X_0=x_0, ..., X_n=x_n)&=P(x_0=A)^{\mathbb{1}_{x_0=A}}.P(x_0=S)^{\mathbb{1}_{x_0=S}}.(1-P(x_0=A)-P(x_0=S))^{\mathbb{1}_{x_0=D}}\\&.P_{A\rightarrow A}^{n_{A\rightarrow A}}.P_{S\rightarrow S}^{n_{S\rightarrow AS}}.P_{D\rightarrow D}^{n_{D\rightarrow D}}.P_{A\rightarrow D}^{n_{A\rightarrow D}}.P_{S\rightarrow D}^{n_{S\rightarrow D}}.P_{D\rightarrow A}^{n_{D\rightarrow A}}.P_{D\rightarrow S}^{n_{D\rightarrow S}}\\
\end{align}

so, the likelihood of a Model 1 sequence $X_0, ..., X_n$ with counts $n_{i\rightarrow j}$ (the number of times a transition from state $i$ to state $j$ has occured in the sequence) is given by:

\begin{align}
L_\theta(X)&=P(x_0=A)^{\mathbb{1}_{x_0=A}}.P(x_0=S)^{\mathbb{1}_{x_0=S}}.(1-P(x_0=A)-P(x_0=S))^{\mathbb{1}_{x_0=D}}\\&.(1-P_{A\rightarrow D})^{n_{A\rightarrow A}}.(1-P_{S\rightarrow D})^{n_{S\rightarrow AS}}.(1-P_{D\rightarrow A}-P_{D\rightarrow S})^{n_{D\rightarrow D}}\\&.P_{A\rightarrow D}^{n_{A\rightarrow D}}.P_{S\rightarrow D}^{n_{S\rightarrow D}}.P_{D\rightarrow A}^{n_{D\rightarrow A}}.P_{D\rightarrow S}^{n_{D\rightarrow S}}\\
\end{align}

the respective log-likelihood becomes:

\begin{align}
l_\theta(X)&=\mathbb{1}_{x_0=A}.log(P(x_0=A)) + \mathbb{1}_{x_0=S}.log(P(x_0=S)) + \mathbb{1}_{x_0=D}.log(1-P(x_0=A)-P(x_0=S))\\&+n_{A\rightarrow A}.log(1-P_{A\rightarrow D})+n_{S\rightarrow AS}.log(1-P_{S\rightarrow D})+n_{D\rightarrow D}.log(1-P_{D\rightarrow A}-P_{D\rightarrow S})\\&+n_{A\rightarrow D}.log(P_{A\rightarrow D})+n_{S\rightarrow D}.log(P_{S\rightarrow D})+n_{D\rightarrow A}.log(P_{D\rightarrow A})+n_{D\rightarrow S}.log(P_{D\rightarrow S})\\
\end{align}

Inspired from [here](https://www.stat.cmu.edu/~cshalizi/dst/20/lectures/19/lecture-19.html#(5)), we can rearrange the log-likelihood for simplicity:

\begin{align}
l_\theta(X)&=log(P(X_0=x_0))+\overset{n-1}{\underset{t=0}{\sum}}log(P_{x_{t}\rightarrow x_{t+1}})\\
&\equiv \overset{n-1}{\underset{t=0}{\sum}}log(P_{x_{t}\rightarrow x_{t+1}})\quad\text{(Conditioned on the initial state $x_0$)}\\
&\equiv -\frac{1}{n}\overset{n-1}{\underset{t=0}{\sum}}log(P_{x_{t}\rightarrow x_{t+1}})\quad\text{(Normalized, and signed for minimization purposes)}\\
\end{align}

We defined $N_{i\rightarrow j}$ the number of times the state $j$ follows the state $i$ in a given process. As such:

\begin{align}
N_i &= \sum_j N_{i\rightarrow j}\\
n &= \sum_i N_i\\
l_\theta(X)&\equiv-\frac{1}{n}\overset{n-1}{\underset{t=0}{\sum}}log(P_{x_{t}\rightarrow x_{t+1}})=-\frac{1}{n}\underset{i, j}{\sum}N_{i\rightarrow j}log(P_{i\rightarrow j})\quad\text{(sum over time vs. sum over state pairs)}
\end{align}

We are now interested in the maximum likelihood estimator (MLE) of the transition $i'\rightarrow j'$.

\begin{align}
\frac{\delta}{\delta P_{i'\rightarrow j'}}l_\theta(X)\equiv&-\frac{N_{i'\rightarrow j'}}{n}.\frac{1}{P_{i'\rightarrow j'}}
\end{align}

With $n$, the number of times the sequence was in state $i'$.

We want to minimize the above formula. As such we find that the simplest answer is:

$$\hat{P}_{i'\rightarrow j'}=\frac{N_{i'\rightarrow j'}}{n}$$

With $n$, the number of times the sequence was in state $i'$.

<u>Maximum likelihood estimator of $P_{D\rightarrow S}$ for model 1:</u>

Given the agnostic formula above, we find that:

$$\hat{P}_{D\rightarrow S}=\frac{N_{D\rightarrow S}}{n}$$
With $n$, the number of times the sequence was in state $i'$.

<u>Maximum likelihood estimator of $P_{D\rightarrow S}$ for model 2:</u>

Given that the above formula is agnostic to the different possible transitions except the one being estimated (i.e. $i'\rightarrow j'$, and in our more specific cases of model 1 and 2: $D\rightarrow S$), the formula would remain unchanged. As such:

$$\hat{P}^{model\,1}_{D\rightarrow S}=\hat{P}^{model\,2}_{D\rightarrow S}=\frac{N_{D\rightarrow S}}{n}$$
With $n$, the number of times the sequence was in state $D$.

### STEP D

We declare our function computing the estimator as stated above.

```{r MLE_estimator}

compute_estimator <- function(sequence, start_state, end_state){
  ### Computes the MLE estimator of the transition probability between
  ### <start_state> and <end_state>. 
  # Computes all transitions in the sequence
  transitions = c()
  for (i in 1:(length(sequence)-1)){
    transitions = cbind(transitions, paste(sequence[i], "-", sequence[i+1]))
  }
  counter = table(transitions)
  # Computes the number of occurences of target transition
  transition_to_check = paste(start_state, "-", end_state)
  n_transition = counter[names(counter)==transition_to_check][[1]]
  # Returns the estimator
  n = sum(counter[substr(names(counter),1,1)==start_state])
  return(round(n_transition/n, 3))
}

```

We now try to compute the estimator convergence for a given transition on model 1, reusing the parameters previously declared in Step B, with an increasing length for the sequence.

We check for the transitions:

\begin{align}
P_{A\rightarrow D} &= 0.4\\
P_{S\rightarrow D} &= 0.2\\
P_{D\rightarrow S} &= 0.4\\
P_{D\rightarrow A} &= 0.4\\
P_{D\rightarrow D} &= 1-P_{D\rightarrow S}-P_{D\rightarrow A}=0.2\\
P_{A\rightarrow A} &= 1 - P_{A\rightarrow D} = 0.6\\
P_{S\rightarrow S} &= 1 - P_{S\rightarrow D} = 0.8\\
\end{align}

```{r MLE_estimator_convergence}

for (n in c(50, 100, 500, 1000, 5000, 10000)) {
  sequence = generate_process_from_model1("A", n, p_AtoD, p_StoD, p_DtoS, p_DtoA)
  cat("== Computing the MLE estimator with n =", n, "==\n")
  pAD = compute_estimator(sequence, "A","D")
  pSD = compute_estimator(sequence, "S","D")
  pDS = compute_estimator(sequence, "D","S")
  pDA = compute_estimator(sequence, "D","A")
  pDD = compute_estimator(sequence, "D","D")
  pAA = compute_estimator(sequence, "A","A")
  pSS = compute_estimator(sequence, "S","S")
  cat(paste("\tReal Probability\tMLE estimator\n"))
  cat(paste("P_AtoD:\t", p_AtoD, "\t\t\t", pAD, "\n"))
  cat(paste("P_StoD:\t", p_StoD, "\t\t\t", pSD, "\n"))
  cat(paste("P_DtoS:\t", p_DtoS, "\t\t\t", pDS, "\n"))
  cat(paste("P_DtoA:\t", p_DtoA, "\t\t\t", pDA, "\n"))
  cat(paste("P_DtoD:\t", 1-p_DtoS-p_DtoA, "\t\t\t", pDD, "\n"))
  cat(paste("P_AtoA:\t", 1-p_AtoD, "\t\t\t", pAA, "\n"))
  cat(paste("P_StoS:\t", 1-p_StoD, "\t\t\t", pSS, "\n\n\n"))
}

```

We empirically (via simulation) evidence that there is convergence.

### STEP E

We would look for transition between states "A" and "S" or "S" and "A", which can only exist in model 2. However this implies that we would have collected enough observations to find those as, depending on the probabilities $P_{A\rightarrow S}$ and $P_{S\rightarrow A}$, as such transitions may not appear when the length of a generated sequence is low.

<hr>

## Question 2 - Maximum likelihood for Poisson processes

We are observing an inhomogeneous Poisson process with intensity $\lambda_0(.)$ on $[0, T_{max}]$. We want
to estimate $\lambda_0(.)$ by a piecewise constant function on a regular partition, which means that the
candidate intensities $\lambda(.)$ are, for a certain integer $d$ giving the number of bins, of the form:

$$\lambda(t)=\underset{i=1}{\overset{d}{\sum}}a^d_i\mathbb{1}_{b_{i-1}^d\le t<b^d_i}$$
With $b_i^d=\frac{i.T_{max}}{d}$

**Step A** -- When $d = 1$, write the likelihood of the process and provide the maximum likelihood estimator of $a^1_1$. Implement it and show on simulation that when $T_{max}$ grows, the estimation is converging when the process is homogeneous.

**Step B** -- For a more general $d$ write the corresponding likelihood and give the maximum likelihood estimator of the different $a^d_i$.

**Step C** -- Simulate by thinning a Poisson process of intensity $ne^{âˆ’x}$ and $n(1 + sin(x))$ on $[0, T_{max}]$ for various $n$. Superpose different MLE of the intensities for various choices of $d$ and $n$.

**Step D** -- Apply AIC criterion to select for a given observed Poisson process a good $d$.

**Step E** -- Apply it on the data of the STAR package, for instance the data citronellal (after having aggregated all the trials into just one point process).

### STEP A

We observe $N$ a Poisson process with conditional intensity:

\begin{align}
\lambda(t)&=\underset{i=1}{\overset{d}{\sum}}a^d_i\mathbb{1}_{b_{i-1}^d\le t<b^d_i}\\
&=a^1_1\quad\text{(When $d=1$)}
\end{align}

As such, we define the log-likelihood as:

\begin{align}
l_\lambda(N)&=\int_0^{T_{max}}log(\lambda(t))dN_t - \int_0^{T_{max}}\lambda(t)dt\\
&=log(a^1_1).N_{[0,T_{max}]}-a^1_1.T_{max}\\
\end{align}

With $N_{[0, T_{max}]}$ the number of points in the bin $d$, i.e., in between 0 and $T_{max}$.

We find the following **log-likelihood**:

\begin{align}
\frac{\delta}{\delta a^1_1}l_\lambda&=\frac{1}{a^1_1}.N_{[0,T_{max}]}-T_{max}\\
&\Rightarrow \hat{a}^1_1=\frac{N_{[0, T_{max}]}}{T_{max}}
\end{align}

From this, we can simulate the process and compute the resulting estimation:

```{r simulation_poisson_process, out.width="100%"}

simulate_a11 <- function(a11, maximum_seconds){
  # Simulates a poisson process with a11 fixed (i.e. homogeneous)
  draws = c()
  maxT = 0
  while (maxT < maximum_seconds) {
    draw = rexp(1, a11)
    maxT = maxT + draw
    draws = c(draws, draw)
  }
  poisson_process = cumsum(draws)
  # Computes the estimator for a growing [0, T] window
  estimators = c()
  for (i in 1:maximum_seconds){
    n_0Tmax = length(poisson_process[poisson_process<i])
    estimators = c(estimators, (n_0Tmax/i))
  }
  plot(x=c(1:maximum_seconds), estimators, type="l", 
       main=paste("Convergence of the MLE estimator of a^1_1=",a11,"\ngiven a",
                  maximum_seconds, "-second-long process", sep=""),
       xlab="Tmax used for estimation", ylab="MLE Estimator Value")
  abline(h=a11, col="red")
  return(list("process"=poisson_process, "estimators"=estimators))
}

# We provide 3 example rates over a maximum span of time of 1 hour
a11 = 1
simulation1 = simulate_a11(a11, 3600)
  
a11 = 0.1
simulation1 = simulate_a11(a11, 3600)

a11= 10
simulation1 = simulate_a11(a11, 3600)

```

We demonstrate convergence of the estimator via simulation. And since the convergence is towards a single value, and given that if an intensity $\lambda$ is constant then the underlying Poisson process is homogeneous, we can conclude that the estimation is converging when the process is homogeneous.

### STEP B

\begin{align}
\lambda(t)&=\underset{i=1}{\overset{d}{\sum}}a^d_i\mathbb{1}_{b_{i-1}^d\le t<b^d_i}\\
l_\lambda(N)&=\int_0^{T_{max}}log(\lambda(t))dN_t - \int_0^{T_{max}}\lambda(t)dt\\
&=\underset{i=1}{\overset{d}{\sum}}log(d_i^d)N_{[b^d_{i-1}, b^d_i]}\\
&=\underset{i=1}{\overset{d}{\sum}}log(d_i^d)N_{[\frac{(i-1)T_{max}}{d}, \frac{iT_{max}}{d}]}-a_i^d\frac{T_{max}}{d}\\
\frac{\delta}{\delta a_i^d}l_\lambda(N)&=\frac{N_{[\frac{(i-1)T_{max}}{d}, \frac{iT_{max}}{d}]}}{a_i^d}-\frac{T_{max}}{d}
\end{align}

As such we find that for a given $d$ number of bins and $i$ bin, we have the MLE estimator of a_i^d:

$$\hat{a}_i^d=\frac{d.N_{[\frac{(i-1)T_{max}}{d}, \frac{iT_{max}}{d}]}}{T_{max}}$$

### STEP C

We start by declaring our rate function and their integrals.

```{r rate_function, out.width="100%"}

rate_function_1 <- function(n, t){
  n*exp(-t)
}

rate_function_2 <- function(n, t){
  n*(1+sin(t))
}

#int_rate_function_1 <- function(n, a, b){
#  -n*exp(-b)+n*exp(-a)
#}

#int_rate_function_2 <- function(n, a, b){
#  n*(b-cos(b)) - n*(a-cos(a))
#}

example_n = 2
x = matrix(seq(1:400)/100)
rf1_y = apply(x, 1, function(x){rate_function_1(example_n,x)})
rf2_y = apply(x, 1, function(x){rate_function_2(example_n,x)})

plot(x, rf1_y, type="l", main=paste("Plot of intensity n*exp(-x)\n",
                                    "on interval [0,4] with n=2"),
     xlab = "Time", ylab="Intensity")
plot(x, rf2_y, type="l", main=paste("Plot of intensity n(1-sin(x))\n",
                                    "on interval [0,4] with n=2"),
     xlab = "Time", ylab="Intensity")

```

We determine that the intensity $\lambda(t)=n*e^{-t}$ is upper-bounded by $N$, we call this upper bound $M_1=n$. We also determine that the intensity $\lambda(t)=n*(1-sin(t))$ is upper-bounded by $N*2$. We call this upper bound $M_2=n*2$. We will use $M_1$ and $M_2$ for our thinning process.

We start with declaring our simulation functions (with thinning process) and MLE_estimator computation function.

```{r thinning_process}

simulate_on_1_interval <- function(n, a, b, M, rate_function) {
  ### Simulate a counting process on a given interval of a Poisson process
  draws = c()
  while (a <= b) {
    # Draws an Exponential random variable
    draw = rexp(1, M)
    a = a+draw
    # Draws a Uniform random variable
    bern = runif(1, 0, M)
    # Checks if the generated point falls below the rate function
    # and keeps it if so
    # records the draw if need be
    if (bern <= rate_function(n, a)){#} && a<=b){
      draws = c(draws, a)
    }
  }
  # Returns the draws
  draws
}

poisson_process_thinning_simulation <- function(n, d, Tmax, M, rate_function) {
  ### Simulate a Poisson process via thinning
  bin_size = Tmax/d
  a = 0
  draws = c()
  bins = c()
  MLE = c()
  while (a<Tmax){
    if (a+bin_size > Tmax){
      upper = Tmax
    } else {
      upper = a + bin_size
    }
    # Draws the Poisson process a hundred times on each given interval
    length_draw = 0
    for (i in 1:100){
      new_draws = simulate_on_1_interval(n, a, upper, M, rate_function)
      draws = c(draws, new_draws)
      length_draw = length_draw + length(new_draws)
    }
    MLE = c(MLE, length_draw/(bin_size*100))
    bins = c(bins, a)
    a = a+bin_size
  }
  # Returns the draws
  return(list(
    "n"=n,
    "d"=d,
    "draws"=draws, 
    "MLE_estimator_a"=MLE,
    "bins"=bins
    )
  )
}

plot_estimator <- function(
  n, d_range, Tmax, M, 
  discretization_factor,
  rate_function, 
  title
) {
  ### Plots the computed estimators given a value n and a range of d values
  ### a M value for the intensity function/rate function
  i = length(d_range)+1
  colors = c(2:(length(d_range)+2))
  for (d in d_range) {
    i = i - 1
    x = matrix(seq(1:(Tmax*discretization_factor))/discretization_factor)
    pp = poisson_process_thinning_simulation(
      n, d, Tmax, M, rate_function
    )
    y = apply(x, 1, function(x){
      if (d == 1){
        pp$MLE_estimator_a
        } else {
          pp$MLE_estimator_a[d-length(pp$bins[pp$bins>=x])]
        }
    })
    if (d == d_range[1]) {
      plot(x, y, type="l", main=title,
           ylim=c(0,M*1.5), col=colors[abs(i-length(d_range))+1])
    } else {
      lines(x, y, col=colors[abs(i-length(d_range))+1])
    }
    legend(
      x="topright",
      legend=apply(matrix(d_range),1, function(x){paste("d =", x)}),
      fill=colors
    )
  }
}

```

```{r}

# Declares hyperparameters
d = c(1, 5, 10, 50, 100)

n = 50
plot_estimator(
  n, d, 10, n, 10, rate_function_1,
  title=paste("MLE Estimation of parameter a given d-windows\n",
        "n=",n,", intensity: n.exp(-x)", sep="")
)

n = 75
plot_estimator(
  n, d, 10, n, 10, rate_function_1,
  title=paste("MLE Estimation of parameter a given d-windows\n",
        "n=",n,", intensity: n.exp(-x)", sep="")
)

n = 100
plot_estimator(
  n, d, 10, n, 10, rate_function_1,
  title=paste("MLE Estimation of parameter a given d-windows\n",
        "n=",n,", intensity: n.exp(-x)", sep="")
)

n = 200
plot_estimator(
  n, d, 10, n, 10, rate_function_1,
  title=paste("MLE Estimation of parameter a given d-windows\n",
        "n=",n,", intensity: n.exp(-x)", sep="")
)

```

```{r}

n = 50
plot_estimator(
  n, d, 10, n*2, 10, rate_function_2,
  title=paste("MLE Estimation of parameter a given d-windows\n",
        "n=",n,", intensity: n.(1-sin(x))", sep="")
)

n = 75
plot_estimator(
  n, d, 10, n*2, 10, rate_function_2,
  title=paste("MLE Estimation of parameter a given d-windows\n",
        "n=",n,", intensity: n.(1-sin(x))", sep="")
)

n = 100
plot_estimator(
  n, d, 10, n*2, 10, rate_function_2,
  title=paste("MLE Estimation of parameter a given d-windows\n",
        "n=",n,", intensity: n.(1-sin(x))", sep="")
)

n = 200
plot_estimator(
  n, d, 10, n*2, 10, rate_function_2,
  title=paste("MLE Estimation of parameter a given d-windows\n",
        "n=",n,", intensity: n.(1-sin(x))", sep="")
)

```

### STEP D

*not completed.*

### STEP E

*not completed.*

<hr>

## Question 3 - How do we check that our simulation is correct?

**Step A** -- Build on Exercise 3 of tutorial 6 to provide a way to simulate a univariate Hawkes process
with intensity:

$$\lambda(t) = v + \underset{T\in N,\,T<t}{\sum}h(t-T)$$

With $h(u)=1-u^2$ if $u\in[0, 1]$, 0 elsewhere.

**Step B** -- By exchanging sums and integrals, show that $\Lambda(t)=\int_0^t\lambda(s)ds$ is equal to:

$$vt + \underset{T\in N,\,T<t}{\sum}\int_T^{min(T+1, t)}(1-(s-T)^2)ds$$
that is

$$vt + \frac{2}{3}N_{t-1}+\underset{T\in N,\,t-1<T<t}{\sum}\big[t-T-\frac{(t-T)^3}{3}\big]$$

With $N_{t-1}$ is the corresponding counting process at time $t-1$.

**Step C** -- Program a function which for a given t computed $\lambda(t)$

**Step D** -- Simulate a Hawkes process thanks to Step A and transform its points with $\Lambda$

**Step E** -- On the transformed points, perform Ogata's tests to verify that we have indeed simulated a Hawkes process

### STEP A

*not completed.*

### STEP B

*not completed.*

### STEP C

*not completed.*

### STEP D

*not completed.*

### STEP E

*not completed.*
