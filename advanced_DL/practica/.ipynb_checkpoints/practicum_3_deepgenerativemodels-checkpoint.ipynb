{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jarvIzm6TMq_"
   },
   "source": [
    "# MSc Data Science: (deep) discriminative models for **MNIST**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z3sguCcpg2Yw"
   },
   "source": [
    "# Loading useful stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xMPYV_R2ghyx"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6062/2291841440.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minitializers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow.keras import initializers\n",
    "\n",
    "tfk = tf.keras\n",
    "tfkl = tf.keras.layers\n",
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O7zTFbzrTaie"
   },
   "source": [
    "#Loading and normalising MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qo-tSpBxTMVq",
    "outputId": "0e1c57b6-0b92-4929-a168-bb5af20a61c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 0s 0us/step\n",
      "11501568/11490434 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(train_images, y_train), (test_images,  y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "train_images = train_images.reshape(train_images.shape[0], 28*28)\n",
    "test_images = test_images.reshape(test_images.shape[0], 28*28)\n",
    "\n",
    "y_train = tf.cast(y_train, tf.int32)\n",
    "y_test =tf.cast(y_test, tf.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "dokv8mqDTMYD",
    "outputId": "dc9c5f38-5f1d-438f-a10a-979a1aea23e7"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[0, :].reshape((28,28)), cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oshsOP18ToYt"
   },
   "outputs": [],
   "source": [
    "# Normalizing the images to the range of [0., 1.]\n",
    "train_images = train_images/255.\n",
    "test_images = test_images/255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m32sDnj6Txzb"
   },
   "source": [
    "# Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LJwgLwUnT23s"
   },
   "source": [
    "Our goal is to build a classifier on MNIST. A first simple example of classifier is **logistic regression**, a particular case of **discriminative model**. The model for (multiclass) logistic regression is \n",
    "$$ p (y | \\mathbf{x} ) = \\text{Cat} (y |\\text{Softmax}(\\mathbf{W}\\mathbf{x}+\\mathbf{b})),$$\n",
    "where the unknown parameters are $\\mathbf{W}$ and $\\mathbf{b}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X-2YQtR9Uiqk"
   },
   "source": [
    "**Question 1.** What are the dimensions of $\\mathbf{W}$ and $\\mathbf{b}$? What is the total number of parameters in the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1UhiLHDU4da"
   },
   "source": [
    "We will build our logistic regression model using [**keras**](https://keras.io/), a nice deep learning API. In particular, keras's [sequential model](https://keras.io/guides/sequential_model/) is simple way of building compositions of parametric functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpTTmZ36T2T9"
   },
   "outputs": [],
   "source": [
    "logistic_regression = tfk.Sequential([\n",
    "  tfkl.InputLayer(input_shape=[28*28,]),\n",
    "  tfkl.Dense(10, kernel_initializer=initializers.RandomNormal(stddev=1)) # because we have 10 classes\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bj4QfIp0vKff"
   },
   "source": [
    "Here, $\\texttt{logistic_regression}$ represents the function $ \\mathbf{x} \\mapsto \\mathbf{W}\\mathbf{x}+\\mathbf{b}$, that takes vectors as inputs, and returns probabilities for each class. We can try with the first MNIST image. The model is initialised by sampling each coefficient of $\\mathbf{W}$ from a standard Gaussian distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nY-1z5qbUuKc",
    "outputId": "e27aab9a-1f2d-4002-8ba6-af2236b1b631"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 10), dtype=float32, numpy=\n",
       "array([[  8.495928  ,   5.488267  ,  20.32993   ,  -3.6566849 ,\n",
       "         -3.0612555 ,   5.8395925 ,  -3.9733167 ,   1.7556076 ,\n",
       "         -0.06998014, -11.785327  ]], dtype=float32)>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(train_images[0:1,])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ijfw4WfpvsP_"
   },
   "source": [
    "Note that the output is a Tensorflow tensor. One can easily get a Numpy array instead this way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KHqYNRStUuMz",
    "outputId": "3074e8cd-01d6-4e23-ed7e-856211d4a01e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8.495928  ,   5.488267  ,  20.32993   ,  -3.6566849 ,\n",
       "         -3.0612555 ,   5.8395925 ,  -3.9733167 ,   1.7556076 ,\n",
       "         -0.06998014, -11.785327  ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(train_images[0:1,]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umvYlfGBv2TX"
   },
   "source": [
    "This $\\texttt{logistic_regression}$ conveniently can also handle **batches** of inputs. Here we look at the predictions of the 10 first digits of MNIST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3OC4ZgXVv1hc",
    "outputId": "82bbfcd8-fe00-4e51-9b93-3c51818f76a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8.4959307e+00,  5.4882679e+00,  2.0329935e+01, -3.6566839e+00,\n",
       "        -3.0612559e+00,  5.8395915e+00, -3.9733174e+00,  1.7556089e+00,\n",
       "        -6.9981225e-02, -1.1785323e+01],\n",
       "       [ 1.2374611e+00,  5.0504885e+00,  1.3919871e+01,  1.8583012e+01,\n",
       "        -7.7279681e-01,  7.7924519e+00,  3.0808744e+00,  6.1116562e+00,\n",
       "        -9.3280048e+00,  4.9655461e+00],\n",
       "       [-7.2871075e+00, -1.5397094e+01, -1.0909403e+01,  7.9048395e+00,\n",
       "         6.6938150e-01, -1.1753743e+01, -1.0543722e+01,  4.1888528e+00,\n",
       "        -1.2670908e+01, -9.9815159e+00],\n",
       "       [ 9.8974733e+00,  5.1582203e+00,  1.2600046e+01,  7.4502740e+00,\n",
       "        -1.0253202e+01, -5.3853106e+00, -1.7099901e+01, -7.8233685e+00,\n",
       "        -2.9905379e+00,  7.6379871e+00],\n",
       "       [-6.8080902e+00, -8.9221420e+00, -1.5757835e+00,  3.5098949e-03,\n",
       "        -1.2371784e+00, -1.3925304e+01, -5.5616784e+00, -6.6310763e-01,\n",
       "        -1.0669029e+01, -1.0636754e+00],\n",
       "       [ 4.1798277e+00,  1.1117022e+00,  1.6029219e+01,  6.1082716e+00,\n",
       "        -7.9128904e+00, -1.2937068e+01, -7.3281417e+00, -2.4463954e+00,\n",
       "         3.0534830e+00,  3.4753108e+00],\n",
       "       [ 8.8484545e+00,  6.3316864e-01,  1.5874753e+00, -3.4516423e+00,\n",
       "        -1.5567224e+01, -9.9952574e+00, -8.9792271e+00, -1.4690780e+01,\n",
       "        -6.3155788e-01, -1.1914693e+01],\n",
       "       [-8.7821283e+00,  2.9177175e+00,  6.4880133e+00,  7.8736238e+00,\n",
       "        -3.1192892e+00, -2.6221457e+00, -6.6073079e+00,  3.6180174e+00,\n",
       "        -6.1136546e+00,  1.2007644e+00],\n",
       "       [ 7.0237122e+00, -1.7929100e-01, -5.9667659e-01, -3.3677773e+00,\n",
       "        -6.6392465e+00, -1.4650140e+01, -2.5470419e+00, -8.1242037e+00,\n",
       "        -8.7048221e-01,  1.6601335e-01],\n",
       "       [ 7.9572091e+00, -5.9142938e+00,  3.9319346e+00,  6.7513628e+00,\n",
       "        -2.2728748e+00, -6.7125788e+00,  2.5103524e+00, -7.0723972e+00,\n",
       "        -1.2723814e+01,  1.8114733e+00]], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_regression(train_images[0:10,]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v65KRekywEtx"
   },
   "source": [
    "One can check that each row of these predictions sums to one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u9dvwvD5UuPQ",
    "outputId": "81d6e570-fb35-4b9f-cc1d-4daf0ca45b28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 19.362774 ,  50.640556 , -65.78042  ,  -0.8083191, -50.422478 ,\n",
       "         3.3333173, -54.16128  ,  -5.146389 , -29.785133 , -11.733627 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(logistic_regression(train_images[0:10,]).numpy(),1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LEUC7LCz_wG"
   },
   "source": [
    "One can us Tensorflow Probability to create the distribution  $p (y | \\mathbf{x} )$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9tGJ3oYdT2WE"
   },
   "outputs": [],
   "source": [
    "p_ygivenx_logistic_regression = tfd.Categorical(logits = logistic_regression(train_images[0:10,]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bHLCouzqwQBW",
    "outputId": "80d10882-b945-47b9-f1cf-4c316ad7a3aa"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([8, 0, 2, 4, 2, 8, 3, 0, 7, 0], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ygivenx_logistic_regression.sample() # sampling the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jCkZytXzwQDl",
    "outputId": "7362d49a-df06-4103-da69-9ceb8b5a79be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=array([0, 0, 3, 8, 8, 8, 3, 0, 8, 0], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_ygivenx_logistic_regression.mode() # looking at the most probable labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mD_RnKqm1W1N"
   },
   "source": [
    "# Training the logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e3E0_KzB1dRE"
   },
   "source": [
    "To train the classifier, we define a function that performs a gradient descent step. First, we choose the flavour of SGD that we want (in this case, the [fairly famous Adam](https://en.wikipedia.org/wiki/Stochastic_gradient_descent#Adam))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgMTlL-2wQFf"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5wus2n2Tubq"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step_logistic_regression(data, labels):\n",
    "  with tf.GradientTape() as tape: # the gradient tape saves all the step that needs to be saved fopr automatic differentiation\n",
    "    p_ygivenx_logistic_regression = tfd.Categorical(logits = logistic_regression(data)) # One could also use logits rather than probs and remove the softmax layer...\n",
    "    logp_ygivenx_logistic_regression = p_ygivenx_logistic_regression.log_prob(labels)\n",
    "    loss = -tf.reduce_mean(logp_ygivenx_logistic_regression)  # the loss is the average negative log likelihood\n",
    "  gradients = tape.gradient(loss, logistic_regression.trainable_variables)  # here, the gradient is automatically computed\n",
    "  optimizer.apply_gradients(zip(gradients, logistic_regression.trainable_variables))  # Adam iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0EmfozUiTueC"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def evaluate_logistic_regression(data, labels):\n",
    "  p_ygivenx_logistic_regression = tfd.Categorical(logits = logistic_regression(data))\n",
    "  logp_ygivenx_logistic_regression = p_ygivenx_logistic_regression.log_prob(labels)\n",
    "  log_likelihood = tf.reduce_mean(logp_ygivenx_logistic_regression)\n",
    "  y_pred = p_ygivenx_logistic_regression.mode()\n",
    "  acc = tf.reduce_mean(tf.cast(y_pred == labels,tf.float32))\n",
    "  return acc, log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sRf2jgK4W9s",
    "outputId": "b5545083-c02d-451f-bbd5-0913a2ddedcc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=float32, numpy=0.09393334>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=-13.758713>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " evaluate_logistic_regression(train_images,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfcpUtie4_MC"
   },
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_images,y_train)).shuffle(60000).batch(32) # TF creates the batches for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G_XF8G4kTugJ",
    "outputId": "6329f2d8-16c6-4e24-a642-72eb9ab8cae8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "Train accuracy  0.1246\n",
      "Test accuracy  0.1305\n",
      "Train log-likelihood  -9.50821\n",
      "Test log-likelihood  -9.32266\n",
      "-----------\n",
      "Epoch  2\n",
      "Train accuracy  0.189433\n",
      "Test accuracy  0.2023\n",
      "Train log-likelihood  -7.02632\n",
      "Test log-likelihood  -6.82459\n",
      "-----------\n",
      "Epoch  3\n",
      "Train accuracy  0.284317\n",
      "Test accuracy  0.2963\n",
      "Train log-likelihood  -5.36089\n",
      "Test log-likelihood  -5.16961\n",
      "-----------\n",
      "Epoch  4\n",
      "Train accuracy  0.380383\n",
      "Test accuracy  0.3898\n",
      "Train log-likelihood  -4.25872\n",
      "Test log-likelihood  -4.0787\n",
      "-----------\n",
      "Epoch  5\n",
      "Train accuracy  0.455667\n",
      "Test accuracy  0.4668\n",
      "Train log-likelihood  -3.50228\n",
      "Test log-likelihood  -3.33061\n",
      "-----------\n",
      "Epoch  6\n",
      "Train accuracy  0.517217\n",
      "Test accuracy  0.5274\n",
      "Train log-likelihood  -2.96103\n",
      "Test log-likelihood  -2.80156\n",
      "-----------\n",
      "Epoch  7\n",
      "Train accuracy  0.565583\n",
      "Test accuracy  0.5775\n",
      "Train log-likelihood  -2.56338\n",
      "Test log-likelihood  -2.4147\n",
      "-----------\n",
      "Epoch  8\n",
      "Train accuracy  0.605117\n",
      "Test accuracy  0.6177\n",
      "Train log-likelihood  -2.26202\n",
      "Test log-likelihood  -2.12532\n",
      "-----------\n",
      "Epoch  9\n",
      "Train accuracy  0.637067\n",
      "Test accuracy  0.6525\n",
      "Train log-likelihood  -2.02775\n",
      "Test log-likelihood  -1.90151\n",
      "-----------\n",
      "Epoch  10\n",
      "Train accuracy  0.664017\n",
      "Test accuracy  0.6769\n",
      "Train log-likelihood  -1.84343\n",
      "Test log-likelihood  -1.72706\n",
      "-----------\n",
      "Epoch  11\n",
      "Train accuracy  0.686133\n",
      "Test accuracy  0.7001\n",
      "Train log-likelihood  -1.69439\n",
      "Test log-likelihood  -1.5864\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "  for images, labels in train_dataset:\n",
    "    train_step_logistic_regression(images, labels) # Adam iteration\n",
    "  acc, log_likelihood = evaluate_logistic_regression(train_images,y_train)\n",
    "  acc_test, log_likelihood_test = evaluate_logistic_regression(test_images,y_test)\n",
    "  print('Epoch  %g' %epoch)\n",
    "  print('Train accuracy  %g' %acc.numpy())\n",
    "  print('Test accuracy  %g' %acc_test.numpy())\n",
    "  print('Train log-likelihood  %g' %log_likelihood.numpy())\n",
    "  print('Test log-likelihood  %g' %log_likelihood_test.numpy())\n",
    "  print('-----------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWZZZwYgz3BK"
   },
   "source": [
    "**Question 2.** Compare the results of your logistic regression classifier with the ones given by scikit-learn's logistic regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6Ju1xtK0Cr8"
   },
   "source": [
    "**Question 3.** Replace the logistic regression model by a deep classifier of your choice (e.g. a MLP or a CNN). Try to beat logistic regression!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Lab_MSc_Data_Science_Deep_Discriminative_Model.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
